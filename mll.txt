# ==========================================================
# Exp 0: Preprocessing (Covariance & Correlation)
# ==========================================================
import pandas as pd
import numpy as np

print("--- Exp 0: Preprocessing ---")
df = pd.DataFrame({'X':[1,2,3,4,5], 'Y':[5,6,7,8,9]})

df.fillna(df.mean(),inplace = True)

cov = np.cov(df['X'],df['Y'])
cor = np.corrcoef(df['X'], df['Y'])

print("Correlation Matrix:\n", cor)
print("Covariance Matrix:\n", cov)


# ==========================================================
# Exp 1: Linear Regression & Multiple Linear Regression
# ==========================================================
print("\n--- Exp 1: Linear Regression ---")
# Linear regression
X = np.array([1,2,3,4,5])
Y = np.array([2,3,5,6,7])

b1 = np.sum((X-X.mean())*(Y-Y.mean()))/np.sum((X-X.mean())**2)
b0 = Y.mean() - b1 * X.mean()
print("Simple Linear Regression: y=", round(b1,2), "x + ", round(b0,2))

# Multiple Linear Regression
X_b = np.c_[np.ones((len(X),1)), X]
# Uses the Normal Equation: theta = (X_T * X)^-1 * X_T * Y
theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ Y

print("Multivariate Theta (Bias, Weight):", theta)


# ==========================================================
# Exp 2: Logistic Regression (Sigmoid Function Demo)
# ==========================================================
print("\n--- Exp 2: Sigmoid Function Demo ---")
# Data
X_lr = np.array([0, 1, 2, 3, 4, 5])

# Assume some weight and bias
b0 = -2.5 # intercept
b1 = 1.0 # slope

# Linear equation
z = b0 + b1 * X_lr

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Apply sigmoid to each z
prob = sigmoid(z)

print("Linear output (z):", np.round(z, 2))
print("Sigmoid output (probabilities):", np.round(prob, 3))
print("Predicted classes:", (prob > 0.5).astype(int))


# ==========================================================
# Exp 3/4: CART Algorithm (Gini Impurity)
# ==========================================================
print("\n--- Exp 3: CART (Gini Impurity) ---")
def gini(y):
    classes, count = np.unique(y, return_counts=True)
    p = count/len(y)
    return 1 - np.sum(p**2)

y_gini = np.array([1,0,0,1,1,0])
left = np.array([1,0,0])
right = np.array([1,1,0])

g_parent = gini(y_gini)
g_left = gini(left)
g_right = gini(right)

gini_split = ((len(left)/len(y_gini))*g_left) + ((len(right)/len(y_gini))*g_right)

gini_gain = g_parent - gini_split

print("Parent gini ",round(g_parent,2), " gini split ", round(gini_split,3), "Gini Gain ", round(gini_gain,3))


# ==========================================================
# Exp 5: Ensemble Learning (Bagging and Boosting)
# ==========================================================
print("\n--- Exp 5: Ensemble (Bagging & Boosting) ---")
# Bagging (Averaging)
def simpleModel(x):
    return 1 + 2.5 * x
def otherModel(x):
    return 0.8 + 3 * x

x_ens = np.array([1,2,3,4,5])
y_ens = (simpleModel(x_ens) + otherModel(x_ens))/2

print("Simple Model Output:", simpleModel(x_ens))
print("Other Model Output:", otherModel(x_ens))
print("Bagging Average:", y_ens)

# Boosting
y_boost = np.array([3,6,9,12])
m1 = y_boost * 0.3
residual = y_boost - m1
m2 = residual * 0.3
residual = y_boost - m2
m3 = residual * 0.3
residual = y_boost - m3
m4 = residual * 0.3

print("M1+M2 Prediction:", m1+m2)
print("M1+M2+M3 Prediction:", m1+m2+m3)
print("M1+M2+M3+M4 Prediction:", m1+m2+m3+m4)


# ==========================================================
# Exp 6 (Part A): Graph-Based Clustering (From Scratch)
# ==========================================================
print("\n--- Exp 6A: Graph-Based Clustering ---")
x_gc = np.array([[1,2],[2,3],[8,9],[3,1],[9,8]])

# Compute pairwise distance matrix
dist = np.sqrt(((x_gc[:,None,:]-x_gc)**2).sum(axis=2))
clusters = {0:[0]} # Initialize with the first point in cluster 0

# Simple graph-based connection logic
for i in range(1,len(x_gc)):
    added = False
    for cid, points in clusters.items():
        for p in points:
            if dist[i][p]<3: # Check distance to existing cluster points
                clusters[cid].append(i)
                added = True
                break
        if added:
            break
    if not added:
        clusters[i] = [i] # Start new cluster

print("Graph Clusters:", clusters)


# ==========================================================
# Exp 6 (Part B): Support Vector Machine (Perceptron-like training)
# ==========================================================
print("\n--- Exp 6B: SVM Training Core ---")
x_svm = np.array([[2,3],[3,4],[4,3],[1,1]])
y_svm = np.array([1,1,1,-1])

w = np.zeros(2)
b = 0
lr = 0.01

for _ in range(1000):
    for i in range(len(x_svm)):
        # Check if point is misclassified or within the margin (y_i * (w.x - b) < 1)
        if y_svm[i]*(np.dot(x_svm[i],w) - b) < 1:
            w += lr * y_svm[i] * x_svm[i] # Update rule for w
            b += lr * y_svm[i]            # Update rule for b

print("Trained weight ", np.round(w,3))
print("Trained bias ", round(b,3))


# ==========================================================
# Exp 7: DBSCAN Algorithm (using Scikit-learn)
# ==========================================================
print("\n--- Exp 7: DBSCAN Clustering ---")
from sklearn.cluster import DBSCAN

X_db = np.array([[1,2],[2,3],[8,9],[6,9],[0,1]])

model = DBSCAN(eps=2,min_samples=2).fit(X_db)

labels = model.labels_
print("Cluster Labels:", labels)

for cluster_id in set(labels):
    if cluster_id == -1:
        print("Noise (ID -1):",X_db[labels == cluster_id])
    else:
        print(f"Cluster {cluster_id}:", X_db[labels == cluster_id])

# ==========================================================
# Exp 8: PCA (SVD - NumPy)
# ==========================================================
print("\n--- Exp 8: PCA (SVD - NumPy) ---")

X_pca = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2]])

X_centered = X_pca - np.mean(X_pca, axis=0)


U, s, Vh = np.linalg.svd(X_centered)
principal_components = Vh

n_components = 1
W = principal_components[:n_components].T # W is the Projection Matrix
X_projected = X_centered @ W

print("Principal Components (Eigenvectors):\n", np.round(principal_components.T, 4))
print("Data Projected to 1D (X_pca):\n", np.round(X_projected, 4))