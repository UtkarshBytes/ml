# --- Experiment 0 ---
# Extract, preprocess, covariance & correlation (pure Python)

data = [
    [1, 2, 3],
    [2, 3, 4],
    [3, 4, 5]
]

def normalize(col):
    mn = min(col)
    mx = max(col)
    return [(x - mn) / (mx - mn) for x in col]

norm_data = [normalize([row[i] for row in data]) for i in range(len(data[0]))]

def mean(lst): return sum(lst)/len(lst)
def covariance(x, y):
    mx, my = mean(x), mean(y)
    return sum((x[i]-mx)*(y[i]-my) for i in range(len(x))) / (len(x)-1)

def correlation(x, y):
    cov = covariance(x, y)
    sx = (sum((xi-mean(x))**2 for xi in x)/(len(x)-1))**0.5
    sy = (sum((yi-mean(y))**2 for yi in y)/(len(y)-1))**0.5
    return cov/(sx*sy)

x, y = [1,2,3,4,5], [2,4,6,8,10]
print("Covariance:", covariance(x, y))
print("Correlation:", correlation(x, y))



# --- Experiment 1 ---
# Linear & Multivariate Linear Regression (pure Python)

x = [1, 2, 3, 4, 5]
y = [2, 4, 5, 4, 5]
mx, my = sum(x)/len(x), sum(y)/len(y)
m = sum((x[i]-mx)*(y[i]-my) for i in range(len(x))) / sum((x[i]-mx)**2 for i in range(len(x)))
c = my - m*mx
print("Linear Regression → y =", round(m, 2), "* x +", round(c, 2))
x_test = 6
y_pred = m*x_test + c
print("Predicted value for x=6 →", round(y_pred, 2))

X = [[1,2], [2,3], [3,4], [4,5]]
y = [2,3,4,5]
b0, b1, b2 = 0, 0, 0
lr = 0.01
for _ in range(1000):
    for i in range(len(X)):
        y_pred = b0 + b1*X[i][0] + b2*X[i][1]
        err = y_pred - y[i]
        b0 -= lr * err
        b1 -= lr * err * X[i][0]
        b2 -= lr * err * X[i][1]
print("Multivariate Coefficients:", round(b0,2), round(b1,2), round(b2,2))



# --- Experiment 2 ---
# Logistic Regression (pure Python)

x = [0, 1, 2, 3, 4]
y = [0, 0, 0, 1, 1]

def sigmoid(z):
    return 1 / (1 + 2.71828**(-z))

w, b = 0.1, 0
lr = 0.1
for _ in range(1000):
    dw, db = 0, 0
    for i in range(len(x)):
        z = w*x[i] + b
        p = sigmoid(z)
        dw += (p - y[i]) * x[i]
        db += (p - y[i])
    w -= lr * dw / len(x)
    b -= lr * db / len(x)

print("Weight:", round(w, 3), "Bias:", round(b, 3))
x_test = 2.5
prob = sigmoid(w*x_test + b)
pred = 1 if prob > 0.5 else 0
print("Predicted:", pred, "| Probability:", round(prob, 2))



# --- Experiment 3 ---
# CART Algorithm (pure Python - single split using Gini index)

data = [[2, 0], [3, 0], [10, 1], [11, 1]]

def gini(groups):
    total = sum(len(g) for g in groups)
    g = 0.0
    for group in groups:
        size = len(group)
        if size == 0: continue
        score = 0.0
        for class_val in [0, 1]:
            p = [row[1] for row in group].count(class_val) / size
            score += p*p
        g += (1 - score) * (size / total)
    return g

best_split, best_gini = None, 1
for t in range(1, 12):
    left = [r for r in data if r[0] <= t]
    right = [r for r in data if r[0] > t]
    g = gini([left, right])
    if g < best_gini:
        best_gini, best_split = g, t
print("Best Split:", best_split, "| Gini:", round(best_gini, 3))



# --- Experiment 4 ---
# Ensemble Learning - Bagging (pure Python concept demo)

data = [1, 2, 3, 4, 5]
for model in range(3):
    sample = []
    for i in range(len(data)):
        sample.append(data[(i + model) % len(data)])
    print("Model", model + 1, "trained on", sample)
votes = [1, 0, 1]
final = 1 if votes.count(1) > votes.count(0) else 0
print("Final Prediction:", final)



# --- Experiment 5 ---
# Simple Linear SVM (pure Python - concept)

x = [1, 2, 3, 4]
y = [-1, -1, 1, 1]
w, b = 0, 0
lr = 0.1
for _ in range(20):
    for i in range(len(x)):
        if y[i]*(w*x[i] + b) < 1:
            w += lr * (y[i]*x[i])
            b += lr * y[i]
        else:
            w -= lr * 0.01 * w
print("Trained Weight:", round(w, 3), "Bias:", round(b, 3))



# --- Experiment 6 ---
# Graph-Based Clustering (pure Python DFS)

graph = {
    0: [1],
    1: [0, 2],
    2: [1],
    3: [4],
    4: [3]
}
visited = set()
def dfs(v):
    visited.add(v)
    for nbr in graph[v]:
        if nbr not in visited:
            dfs(nbr)
cluster = 0
for v in graph:
    if v not in visited:
        cluster += 1
        dfs(v)
        print("Cluster", cluster, "formed")



# --- Experiment 7 ---
# DBSCAN (pure Python simplified)

points = [(1,2), (2,2), (2,3), (10,10)]
eps, minPts = 2, 2
def dist(a, b):
    return ((a[0]-b[0])**2 + (a[1]-b[1])**2)**0.5
for p in points:
    neighbors = [q for q in points if dist(p, q) <= eps]
    if len(neighbors) >= minPts:
        print(p, "→ Core Point with neighbors:", neighbors)
    else:
        print(p, "→ Noise or Border Point")



# --- Experiment 8 ---
# PCA - Find covariance and reduce 2D -> 1D (pure Python)

data = [[2,3], [3,4], [4,5]]
mx = sum(i[0] for i in data)/len(data)
my = sum(i[1] for i in data)/len(data)
cov_xx = sum((i[0]-mx)**2 for i in data)/len(data)
cov_yy = sum((i[1]-my)**2 for i in data)/len(data)
cov_xy = sum((i[0]-mx)*(i[1]-my) for i in data)/len(data)
print("Covariance Matrix:")
print("[", round(cov_xx,2), round(cov_xy,2), "]")
print("[", round(cov_xy,2), round(cov_yy,2), "]")
print("Dominant Direction ≈ [1, 1] → 1D Projection")
